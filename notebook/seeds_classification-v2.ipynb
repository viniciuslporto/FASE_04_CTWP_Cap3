{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifica√ß√£o de Variedades de Gr√£os de Trigo\n",
    "## Aplica√ß√£o da Metodologia CRISP-DM\n",
    "\n",
    "**Integrantes do Grupo:**\n",
    "- Vinicius de Santana Gama - RM566672\n",
    "- Pedro Carvalho Rocha Lima - RM567330\n",
    "- Vinicius Lisboa Porto - RM561406\n",
    "- Marlon Paulino Marinho - RM566793\n",
    "- Danilo Marques Dantas - RM567583\n",
    "\n",
    "---\n",
    "\n",
    "## Contexto do Problema\n",
    "\n",
    "Em cooperativas agr√≠colas de pequeno porte, a classifica√ß√£o manual de gr√£os √© demorada e sujeita a erros humanos. Este projeto desenvolve um modelo de aprendizado de m√°quina para automatizar a classifica√ß√£o de tr√™s variedades de trigo (Kama, Rosa e Canadian) com base em caracter√≠sticas f√≠sicas dos gr√£os.\n",
    "\n",
    "**Objetivo:** Aplicar CRISP-DM para desenvolver, comparar e otimizar modelos de classifica√ß√£o que auxiliem cooperativas agr√≠colas na automa√ß√£o do processo de classifica√ß√£o de gr√£os."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ IMPORTANTE: Upload do Dataset\n",
    "\n",
    "**ANTES DE CONTINUAR:**\n",
    "1. Clique no √≠cone de **pasta** üìÅ no menu lateral esquerdo\n",
    "2. Clique no √≠cone de **upload** (seta para cima)\n",
    "3. Selecione o arquivo `seeds_dataset.txt`\n",
    "4. Aguarde o upload completar\n",
    "5. Execute as c√©lulas abaixo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√£o de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipula√ß√£o de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pr√©-processamento\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Algoritmos de classifica√ß√£o\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# M√©tricas de avalia√ß√£o\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Ignorar warnings desnecess√°rios\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Bibliotecas importadas com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Explora√ß√£o Inicial dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defini√ß√£o dos nomes das colunas conforme documenta√ß√£o\n",
    "column_names = [\n",
    "    'Area',\n",
    "    'Perimetro',\n",
    "    'Compacidade',\n",
    "    'Comprimento_Nucleo',\n",
    "    'Largura_Nucleo',\n",
    "    'Coef_Assimetria',\n",
    "    'Comprimento_Sulco',\n",
    "    'Variedade'\n",
    "]\n",
    "\n",
    "# Buscar arquivo em poss√≠veis localiza√ß√µes\n",
    "possible_paths = [\n",
    "    'seeds_dataset.txt',  # Diret√≥rio atual\n",
    "    '/content/seeds_dataset.txt',  # Raiz do Colab\n",
    "    'sample_data/seeds_dataset.txt'  # Pasta sample_data\n",
    "]\n",
    "\n",
    "file_path = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        file_path = path\n",
    "        print(f\"‚úì Arquivo encontrado em: {path}\")\n",
    "        break\n",
    "\n",
    "if file_path is None:\n",
    "    print(\"‚ùå ERRO: Arquivo seeds_dataset.txt n√£o encontrado!\")\n",
    "    print(\"\\nüìã INSTRU√á√ïES:\")\n",
    "    print(\"1. Clique no √≠cone de PASTA üìÅ no menu lateral esquerdo\")\n",
    "    print(\"2. Clique no √≠cone de UPLOAD (seta para cima)\")\n",
    "    print(\"3. Selecione o arquivo 'seeds_dataset.txt'\")\n",
    "    print(\"4. Aguarde o upload completar\")\n",
    "    print(\"5. Execute esta c√©lula novamente\")\n",
    "    raise FileNotFoundError(\"seeds_dataset.txt n√£o encontrado\")\n",
    "\n",
    "# Carregamento do dataset\n",
    "# Usando delimitador de espa√ßos em branco (tabs ou espa√ßos m√∫ltiplos)\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep='\\s+',  # Aceita um ou mais espa√ßos/tabs\n",
    "    names=column_names,\n",
    "    header=None,\n",
    "    engine='python'  # Necess√°rio para usar regex no sep\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Dataset carregado com sucesso!\")\n",
    "print(f\"Dimens√µes: {df.shape[0]} amostras x {df.shape[1]} colunas\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PRIMEIRAS LINHAS DO DATASET\")\n",
    "print(f\"{'='*60}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa√ß√µes gerais do dataset\n",
    "print(\"INFORMA√á√ïES GERAIS DO DATASET\")\n",
    "print(f\"{'='*60}\")\n",
    "df.info()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DISTRIBUI√á√ÉO DAS CLASSES\")\n",
    "print(f\"{'='*60}\")\n",
    "print(df['Variedade'].value_counts().sort_index())\n",
    "print(f\"\\nClasses balanceadas: {df['Variedade'].value_counts().std() < 5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamento das variedades\n",
    "variedades_map = {1: 'Kama', 2: 'Rosa', 3: 'Canadian'}\n",
    "print(\"\\nMapeamento das Variedades:\")\n",
    "for key, value in variedades_map.items():\n",
    "    count = (df['Variedade'] == key).sum()\n",
    "    print(f\"  {key} = {value} ({count} amostras)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An√°lise Explorat√≥ria de Dados (EDA)\n",
    "\n",
    "### 3.1 Estat√≠sticas Descritivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas descritivas completas\n",
    "print(\"ESTAT√çSTICAS DESCRITIVAS\")\n",
    "print(f\"{'='*60}\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica√ß√£o de valores ausentes\n",
    "print(\"VERIFICA√á√ÉO DE VALORES AUSENTES\")\n",
    "print(f\"{'='*60}\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing)\n",
    "print(f\"\\n‚úì Total de valores ausentes: {missing.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualiza√ß√£o da Distribui√ß√£o das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramas das features\n",
    "features = df.columns[:-1]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(features):\n",
    "    axes[idx].hist(df[col], bins=25, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribui√ß√£o: {col}', fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequ√™ncia')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Remover subplots vazios\n",
    "fig.delaxes(axes[7])\n",
    "fig.delaxes(axes[8])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribui√ß√£o das Caracter√≠sticas dos Gr√£os', fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 An√°lise de Outliers com Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots para identificar outliers\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(features):\n",
    "    axes[idx].boxplot(df[col], vert=True, patch_artist=True,\n",
    "                     boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                     medianprops=dict(color='red', linewidth=2))\n",
    "    axes[idx].set_title(f'Boxplot: {col}', fontweight='bold')\n",
    "    axes[idx].set_ylabel(col)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "fig.delaxes(axes[7])\n",
    "fig.delaxes(axes[8])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('An√°lise de Outliers - Boxplots', fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Matriz de Correla√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correla√ß√£o\n",
    "correlation_matrix = df[features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            square=True, linewidths=0.5, cbar_kws={'shrink': 0.8})\n",
    "plt.title('Matriz de Correla√ß√£o entre Features', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificar correla√ß√µes fortes\n",
    "print(\"\\nCORRELA√á√ïES FORTES (|r| > 0.8):\")\n",
    "print(f\"{'='*60}\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            print(f\"{correlation_matrix.columns[i]:25} <-> {correlation_matrix.columns[j]:25} = {correlation_matrix.iloc[i, j]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Scatter Plots - Rela√ß√µes entre Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots das principais rela√ß√µes\n",
    "principais_relacoes = [\n",
    "    ('Area', 'Perimetro'),\n",
    "    ('Comprimento_Nucleo', 'Largura_Nucleo'),\n",
    "    ('Area', 'Compacidade'),\n",
    "    ('Perimetro', 'Comprimento_Sulco')\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "cores = {1: 'red', 2: 'blue', 3: 'green'}\n",
    "labels = {1: 'Kama', 2: 'Rosa', 3: 'Canadian'}\n",
    "\n",
    "for idx, (feat1, feat2) in enumerate(principais_relacoes):\n",
    "    for variedade in [1, 2, 3]:\n",
    "        mask = df['Variedade'] == variedade\n",
    "        axes[idx].scatter(df[mask][feat1], df[mask][feat2],\n",
    "                         c=cores[variedade], label=labels[variedade],\n",
    "                         alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    axes[idx].set_xlabel(feat1, fontweight='bold')\n",
    "    axes[idx].set_ylabel(feat2, fontweight='bold')\n",
    "    axes[idx].set_title(f'{feat1} vs {feat2}', fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Rela√ß√µes entre Features por Variedade', fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Distribui√ß√£o de Features por Variedade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots por variedade\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(features):\n",
    "    data_by_variety = [df[df['Variedade'] == v][col].values for v in [1, 2, 3]]\n",
    "    \n",
    "    bp = axes[idx].boxplot(data_by_variety, labels=['Kama', 'Rosa', 'Canadian'],\n",
    "                           patch_artist=True)\n",
    "    \n",
    "    for patch, color in zip(bp['boxes'], ['red', 'blue', 'green']):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    \n",
    "    axes[idx].set_title(f'{col} por Variedade', fontweight='bold')\n",
    "    axes[idx].set_ylabel(col)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "fig.delaxes(axes[7])\n",
    "fig.delaxes(axes[8])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribui√ß√£o das Features por Variedade de Trigo', fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pr√©-processamento dos Dados\n",
    "\n",
    "### 4.1 Separa√ß√£o de Features e Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa√ß√£o de features (X) e target (y)\n",
    "X = df.drop('Variedade', axis=1)\n",
    "y = df['Variedade']\n",
    "\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")\n",
    "print(f\"\\nDistribui√ß√£o do target:\")\n",
    "print(y.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Divis√£o em Conjuntos de Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divis√£o estratificada 70/30\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Conjunto de Treino: {X_train.shape[0]} amostras\")\n",
    "print(f\"Conjunto de Teste: {X_test.shape[0]} amostras\")\n",
    "print(f\"\\nDistribui√ß√£o no Treino:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(f\"\\nDistribui√ß√£o no Teste:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Padroniza√ß√£o das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padroniza√ß√£o usando StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Converter de volta para DataFrame para facilitar an√°lise\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "print(\"‚úì Dados padronizados com sucesso\")\n",
    "print(f\"\\nEstat√≠sticas ap√≥s padroniza√ß√£o (Treino):\")\n",
    "print(X_train_scaled_df.describe().loc[['mean', 'std']].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementa√ß√£o dos Modelos de Classifica√ß√£o\n",
    "\n",
    "### 5.1 Defini√ß√£o dos Modelos Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicion√°rio de modelos base\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "print(\"MODELOS DEFINIDOS:\")\n",
    "print(f\"{'='*60}\")\n",
    "for name in models.keys():\n",
    "    print(f\"  ‚Ä¢ {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Treinamento e Avalia√ß√£o dos Modelos Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento e avalia√ß√£o\n",
    "results = {}\n",
    "\n",
    "print(\"TREINAMENTO DOS MODELOS BASE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Treinando {name}...\")\n",
    "    \n",
    "    # Treinar\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predi√ß√µes\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # M√©tricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Valida√ß√£o cruzada\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚úì Acur√°cia: {accuracy:.4f}\")\n",
    "    print(f\"  ‚úì CV Score: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úì Todos os modelos treinados com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Compara√ß√£o de Desempenho dos Modelos Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame com m√©tricas\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Modelo': list(results.keys()),\n",
    "    'Acur√°cia': [results[m]['accuracy'] for m in results],\n",
    "    'Precis√£o': [results[m]['precision'] for m in results],\n",
    "    'Recall': [results[m]['recall'] for m in results],\n",
    "    'F1-Score': [results[m]['f1_score'] for m in results],\n",
    "    'CV Mean': [results[m]['cv_mean'] for m in results],\n",
    "    'CV Std': [results[m]['cv_std'] for m in results]\n",
    "})\n",
    "\n",
    "metrics_df = metrics_df.sort_values('Acur√°cia', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"COMPARA√á√ÉO DE DESEMPENHO - MODELOS BASE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o comparativa\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Gr√°fico 1: Todas as m√©tricas\n",
    "metrics_plot = metrics_df.set_index('Modelo')[['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score']]\n",
    "metrics_plot.plot(kind='bar', ax=axes[0], width=0.8)\n",
    "axes[0].set_title('Compara√ß√£o de M√©tricas - Modelos Base', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_xlabel('Modelo')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].set_ylim([0.85, 1.0])\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico 2: Valida√ß√£o Cruzada\n",
    "x_pos = np.arange(len(metrics_df))\n",
    "axes[1].bar(x_pos, metrics_df['CV Mean'], yerr=metrics_df['CV Std'],\n",
    "           capsize=5, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(metrics_df['Modelo'], rotation=45, ha='right')\n",
    "axes[1].set_title('Valida√ß√£o Cruzada (5-Fold)', fontweight='bold', fontsize=12)\n",
    "axes[1].set_ylabel('Acur√°cia M√©dia')\n",
    "axes[1].set_xlabel('Modelo')\n",
    "axes[1].set_ylim([0.85, 1.0])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Matrizes de Confus√£o dos Modelos Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar matrizes de confus√£o\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, result['y_pred'])\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm,\n",
    "        display_labels=['Kama', 'Rosa', 'Canadian']\n",
    "    )\n",
    "    \n",
    "    disp.plot(ax=axes[idx], cmap='Blues', colorbar=False)\n",
    "    axes[idx].set_title(f'{name}\\nAcur√°cia: {result[\"accuracy\"]:.4f}',\n",
    "                       fontweight='bold', fontsize=11)\n",
    "    axes[idx].grid(False)\n",
    "\n",
    "# Remover subplot vazio\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Matrizes de Confus√£o - Modelos Base', fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Relat√≥rios de Classifica√ß√£o Detalhados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relat√≥rios detalhados por classe\n",
    "print(\"RELAT√ìRIOS DE CLASSIFICA√á√ÉO DETALHADOS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "target_names = ['Kama', 'Rosa', 'Canadian']\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(classification_report(y_test, result['y_pred'],\n",
    "                               target_names=target_names,\n",
    "                               digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Otimiza√ß√£o de Hiperpar√¢metros\n",
    "\n",
    "### 6.1 Defini√ß√£o dos Grids de Hiperpar√¢metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grids de hiperpar√¢metros para otimiza√ß√£o\n",
    "param_grids = {\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear'],\n",
    "        'penalty': ['l2']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"GRIDS DE HIPERPAR√ÇMETROS DEFINIDOS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "for model_name, params in param_grids.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    for param, values in params.items():\n",
    "        print(f\"  ‚Ä¢ {param}: {values}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Execu√ß√£o do GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch para otimiza√ß√£o\n",
    "optimized_results = {}\n",
    "\n",
    "print(\"OTIMIZA√á√ÉO DE HIPERPAR√ÇMETROS (GridSearchCV)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for name in param_grids.keys():\n",
    "    print(f\"Otimizando {name}...\")\n",
    "    \n",
    "    # Criar novo modelo\n",
    "    if name == 'KNN':\n",
    "        base_model = KNeighborsClassifier()\n",
    "    elif name == 'SVM':\n",
    "        base_model = SVC(random_state=42)\n",
    "    elif name == 'Random Forest':\n",
    "        base_model = RandomForestClassifier(random_state=42)\n",
    "    elif name == 'Logistic Regression':\n",
    "        base_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    \n",
    "    # GridSearch\n",
    "    grid_search = GridSearchCV(\n",
    "        base_model,\n",
    "        param_grids[name],\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Melhor modelo\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_opt = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # M√©tricas\n",
    "    accuracy_opt = accuracy_score(y_test, y_pred_opt)\n",
    "    precision_opt = precision_score(y_test, y_pred_opt, average='weighted')\n",
    "    recall_opt = recall_score(y_test, y_pred_opt, average='weighted')\n",
    "    f1_opt = f1_score(y_test, y_pred_opt, average='weighted')\n",
    "    \n",
    "    # Armazenar\n",
    "    optimized_results[name] = {\n",
    "        'model': best_model,\n",
    "        'y_pred': y_pred_opt,\n",
    "        'accuracy': accuracy_opt,\n",
    "        'precision': precision_opt,\n",
    "        'recall': recall_opt,\n",
    "        'f1_score': f1_opt,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'cv_score': grid_search.best_score_,\n",
    "        'improvement': accuracy_opt - results[name]['accuracy']\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚úì Melhor Score CV: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"  ‚úì Acur√°cia Teste: {accuracy_opt:.4f}\")\n",
    "    print(f\"  ‚úì Melhoria: {optimized_results[name]['improvement']:+.4f}\")\n",
    "    print(f\"  ‚úì Melhores Par√¢metros: {grid_search.best_params_}\")\n",
    "    print()\n",
    "\n",
    "# Naive Bayes n√£o precisa otimiza√ß√£o (sem hiperpar√¢metros relevantes)\n",
    "optimized_results['Naive Bayes'] = results['Naive Bayes'].copy()\n",
    "optimized_results['Naive Bayes']['best_params'] = 'N/A'\n",
    "optimized_results['Naive Bayes']['improvement'] = 0.0\n",
    "\n",
    "print(\"‚úì Otimiza√ß√£o conclu√≠da\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Compara√ß√£o: Modelos Base vs Otimizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame comparativo\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Modelo': list(optimized_results.keys()),\n",
    "    'Acur√°cia Base': [results[m]['accuracy'] for m in optimized_results],\n",
    "    'Acur√°cia Otimizada': [optimized_results[m]['accuracy'] for m in optimized_results],\n",
    "    'Melhoria': [optimized_results[m]['improvement'] for m in optimized_results],\n",
    "    'F1-Score Otimizado': [optimized_results[m]['f1_score'] for m in optimized_results]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('Acur√°cia Otimizada', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"COMPARA√á√ÉO: MODELOS BASE vs OTIMIZADOS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o da compara√ß√£o\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison_df['Acur√°cia Base'],\n",
    "               width, label='Base', alpha=0.8, color='lightcoral')\n",
    "bars2 = ax.bar(x + width/2, comparison_df['Acur√°cia Otimizada'],\n",
    "               width, label='Otimizado', alpha=0.8, color='lightgreen')\n",
    "\n",
    "ax.set_xlabel('Modelo', fontweight='bold')\n",
    "ax.set_ylabel('Acur√°cia', fontweight='bold')\n",
    "ax.set_title('Compara√ß√£o: Modelos Base vs Otimizados', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Modelo'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim([0.85, 1.0])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.3f}',\n",
    "               ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Matrizes de Confus√£o - Modelos Otimizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrizes de confus√£o dos modelos otimizados\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, result) in enumerate(optimized_results.items()):\n",
    "    cm = confusion_matrix(y_test, result['y_pred'])\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm,\n",
    "        display_labels=['Kama', 'Rosa', 'Canadian']\n",
    "    )\n",
    "    \n",
    "    disp.plot(ax=axes[idx], cmap='Greens', colorbar=False)\n",
    "    axes[idx].set_title(f'{name} (Otimizado)\\nAcur√°cia: {result[\"accuracy\"]:.4f}',\n",
    "                       fontweight='bold', fontsize=11)\n",
    "    axes[idx].grid(False)\n",
    "\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Matrizes de Confus√£o - Modelos Otimizados', fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. An√°lise de Import√¢ncia das Features (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance do Random Forest otimizado\n",
    "rf_model = optimized_results['Random Forest']['model']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Import√¢ncia': rf_model.feature_importances_\n",
    "}).sort_values('Import√¢ncia', ascending=False)\n",
    "\n",
    "print(\"IMPORT√ÇNCIA DAS FEATURES (Random Forest Otimizado)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(feature_importance.to_string(index=False))\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Import√¢ncia'],\n",
    "         color='forestgreen', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Import√¢ncia', fontweight='bold')\n",
    "plt.ylabel('Feature', fontweight='bold')\n",
    "plt.title('Import√¢ncia das Features - Random Forest', fontweight='bold', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sele√ß√£o do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar o melhor modelo\n",
    "best_model_name = comparison_df.iloc[0]['Modelo']\n",
    "best_model_info = optimized_results[best_model_name]\n",
    "\n",
    "print(\"MELHOR MODELO IDENTIFICADO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Modelo: {best_model_name}\")\n",
    "print(f\"Acur√°cia: {best_model_info['accuracy']:.4f}\")\n",
    "print(f\"Precis√£o: {best_model_info['precision']:.4f}\")\n",
    "print(f\"Recall: {best_model_info['recall']:.4f}\")\n",
    "print(f\"F1-Score: {best_model_info['f1_score']:.4f}\")\n",
    "if best_model_info['best_params'] != 'N/A':\n",
    "    print(f\"\\nMelhores Hiperpar√¢metros:\")\n",
    "    for param, value in best_model_info['best_params'].items():\n",
    "        print(f\"  ‚Ä¢ {param}: {value}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interpreta√ß√£o dos Resultados e Insights\n",
    "\n",
    "### 9.1 An√°lise de Erros do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de erros\n",
    "best_predictions = best_model_info['y_pred']\n",
    "errors = y_test != best_predictions\n",
    "error_indices = np.where(errors)[0]\n",
    "\n",
    "print(f\"AN√ÅLISE DE ERROS - {best_model_name}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total de erros: {errors.sum()} de {len(y_test)} ({errors.sum()/len(y_test)*100:.2f}%)\")\n",
    "print(f\"\\nDistribui√ß√£o dos erros:\")\n",
    "\n",
    "if errors.sum() > 0:\n",
    "    error_df = pd.DataFrame({\n",
    "        'Real': y_test.iloc[error_indices].map(variedades_map),\n",
    "        'Predito': pd.Series(best_predictions[error_indices]).map(variedades_map)\n",
    "    })\n",
    "    print(error_df.groupby(['Real', 'Predito']).size().to_frame('Quantidade'))\n",
    "else:\n",
    "    print(\"Nenhum erro - Classifica√ß√£o perfeita!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Resumo Executivo dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMO EXECUTIVO - CLASSIFICA√á√ÉO DE GR√ÉOS DE TRIGO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET:\")\n",
    "print(f\"   ‚Ä¢ Total de amostras: {len(df)}\")\n",
    "print(f\"   ‚Ä¢ Features analisadas: {len(features)}\")\n",
    "print(f\"   ‚Ä¢ Classes: 3 (Kama, Rosa, Canadian)\")\n",
    "print(f\"   ‚Ä¢ Divis√£o: 70% treino / 30% teste\")\n",
    "\n",
    "print(\"\\n2. MODELOS TESTADOS:\")\n",
    "for name in models.keys():\n",
    "    print(f\"   ‚Ä¢ {name}\")\n",
    "\n",
    "print(\"\\n3. DESEMPENHO (Top 3 Modelos Otimizados):\")\n",
    "for idx, row in comparison_df.head(3).iterrows():\n",
    "    print(f\"   {idx+1}. {row['Modelo']}: {row['Acur√°cia Otimizada']:.4f} (melhoria: {row['Melhoria']:+.4f})\")\n",
    "\n",
    "print(f\"\\n4. MELHOR MODELO: {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ Acur√°cia: {best_model_info['accuracy']:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score: {best_model_info['f1_score']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Taxa de erro: {(1-best_model_info['accuracy'])*100:.2f}%\")\n",
    "\n",
    "print(\"\\n5. FEATURES MAIS IMPORTANTES (Random Forest):\")\n",
    "for idx, row in feature_importance.head(3).iterrows():\n",
    "    print(f\"   {idx+1}. {row['Feature']}: {row['Import√¢ncia']:.4f}\")\n",
    "\n",
    "print(\"\\n6. PRINCIPAIS INSIGHTS:\")\n",
    "print(\"   ‚Ä¢ Dataset bem balanceado permite modelos robustos\")\n",
    "print(\"   ‚Ä¢ Alta correla√ß√£o entre Area e Perimetro (esperado geometricamente)\")\n",
    "print(\"   ‚Ä¢ Separabilidade clara entre variedades nas features morfol√≥gicas\")\n",
    "print(\"   ‚Ä¢ Otimiza√ß√£o de hiperpar√¢metros trouxe melhorias consistentes\")\n",
    "print(f\"   ‚Ä¢ {best_model_name} apresenta melhor equil√≠brio precis√£o/generaliza√ß√£o\")\n",
    "\n",
    "print(\"\\n7. APLICABILIDADE PR√ÅTICA:\")\n",
    "print(\"   ‚Ä¢ Modelo pronto para automa√ß√£o em cooperativas agr√≠colas\")\n",
    "print(\"   ‚Ä¢ Redu√ß√£o significativa de tempo vs classifica√ß√£o manual\")\n",
    "print(\"   ‚Ä¢ Minimiza√ß√£o de erros humanos no processo\")\n",
    "print(\"   ‚Ä¢ Possibilidade de integra√ß√£o com sistemas de vis√£o computacional\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclus√µes\n",
    "\n",
    "### Principais Descobertas:\n",
    "\n",
    "1. **Qualidade do Dataset**: O conjunto de dados Seeds apresenta excelente qualidade, com distribui√ß√£o balanceada entre as tr√™s variedades e aus√™ncia de valores faltantes.\n",
    "\n",
    "2. **Separabilidade das Classes**: As caracter√≠sticas f√≠sicas dos gr√£os (especialmente √°rea, per√≠metro e compacidade) proporcionam boa separabilidade entre as variedades Kama, Rosa e Canadian.\n",
    "\n",
    "3. **Desempenho dos Modelos**: Todos os algoritmos testados apresentaram desempenho superior a 90%, demonstrando que o problema √© bem adequado para t√©cnicas de aprendizado de m√°quina.\n",
    "\n",
    "4. **Otimiza√ß√£o Efetiva**: A otimiza√ß√£o de hiperpar√¢metros via GridSearchCV resultou em melhorias consistentes, especialmente para KNN e SVM.\n",
    "\n",
    "5. **Features Importantes**: As caracter√≠sticas geom√©tricas (√°rea, per√≠metro, compacidade) s√£o os principais discriminadores entre as variedades.\n",
    "\n",
    "### Recomenda√ß√µes para Cooperativas Agr√≠colas:\n",
    "\n",
    "1. **Implementa√ß√£o Gradual**: Iniciar com valida√ß√£o paralela (manual + autom√°tica) para ganhar confian√ßa no sistema.\n",
    "\n",
    "2. **Coleta de Dados Cont√≠nua**: Manter registro das classifica√ß√µes para retreinamento peri√≥dico dos modelos.\n",
    "\n",
    "3. **Integra√ß√£o com Vis√£o Computacional**: Combinar o modelo com sistemas de captura de imagem para automa√ß√£o completa.\n",
    "\n",
    "4. **Monitoramento de Desempenho**: Estabelecer m√©tricas de acompanhamento para detectar degrada√ß√£o do modelo.\n",
    "\n",
    "### Trabalhos Futuros:\n",
    "\n",
    "1. Testar Deep Learning (CNNs) com imagens dos gr√£os\n",
    "2. Expandir para outras variedades de trigo\n",
    "3. Desenvolver interface web para uso pr√°tico\n",
    "4. Estudar classifica√ß√£o em tempo real com edge computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Refer√™ncias\n",
    "\n",
    "- UCI Machine Learning Repository: Seeds Dataset\n",
    "- Scikit-learn Documentation\n",
    "- CRISP-DM Methodology"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
